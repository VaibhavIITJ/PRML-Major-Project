# -*- coding: utf-8 -*-
"""PRML_Major_Project_US_Accidents.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BhPuJDj_RSfXV3_h2dimoSkVHOAq5KfX

# Exploring the data
"""

import pandas as pd
pd.options.mode.chained_assignment = None
pd.options.display.max_columns = 999
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
# import all necesary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.patches as mpatches
# %matplotlib inline
import seaborn as sns
import calendar
import plotly as pt
from plotly import graph_objs as go
import plotly.express as px
import plotly.figure_factory as ff
from pylab import *
import matplotlib.patheffects as PathEffects


import warnings
warnings.filterwarnings('ignore')

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import label_binarize


import plotly.graph_objects as go
from nltk.corpus import stopwords

df = pd.read_csv("/content/drive/MyDrive/US_Accidents_Dec21_updated.csv")

from google.colab import drive
drive.mount('/content/drive')

df.head(3)

df.isnull().sum()

"""# EDA

## States
"""

df.State

states = df.State.unique()
len(states)

states_by_accident = df.State.value_counts()
states_by_accident[:5]

fig, ax = plt.subplots(figsize = (20,5))
c = sns.countplot(x="State", data=df, orient = 'v', palette = "crest_r", order = df['State'].value_counts().index)
c.set_title("States with No. of Accidents");

"""- Among the top 100 cities in number of accidents, which states do they belong to most frequently.

- The cities with most number of accidents are California, Florida, Texas, Oregon, Virginia.

- California (CA) is the 3rd most largest state of US after Texas (TX) and Alaska (AL)
- Also California (CA) is the most populated among all, followed by Texas (TX)
- Alaska (AL) is the largest state but least populated state at 48th rank
"""

max_acc_state=df['State'].value_counts().head(10)

plt.figure(figsize=(10,6))
plt.pie(max_acc_state,labels=max_acc_state.index,autopct='%.2f%%')
plt.title("States in US reported most accident cases",size=17,color="grey")

df_st_ct = pd.value_counts(df['State'])

fig = go.Figure(data=go.Choropleth(
    locations=df_st_ct.index,
    z = df_st_ct.values.astype(float),  # Data to be color-coded
    locationmode = 'USA-states',     # set of locations match entries in `locations`
    colorscale = 'YlOrRd',
    colorbar_title = "Count",
))

fig.update_layout(
    title_text = 'US Accidents by State',
    geo_scope='usa', # limite map scope to USA
)

fig.show()

"""## City"""

cities = df.City.unique()
len(cities)
cities

cities_by_accident = df.City.value_counts()
cities_by_accident[:20]

cities_20_by_accident=df["City"].value_counts().sort_values()[-20:].reset_index()
cities_20_by_accident.columns=["city","number_of_accidents"]

plt.figure(figsize=(20,7))
sns.barplot(x="city",y="number_of_accidents",data=cities_20_by_accident , palette='Set3')
plt.title("TOP 20 CITIES WITH HIGHEST NUMBER OF ACCIDENTS",fontsize=20)
plt.xticks(rotation=40)
plt.show()

"""- There is quite a lot of presence of cities from Florida (Miami, Orlando)followed by California (LA, Sacramento, San Diego, Jacksonville), Texas (Houston, Dallas, Austin) , and North Carolina (Charlotte,Raleigh). This is in tandem with the top states: Florida,California, Texas, and North Carolina."""

cities_by_accident[:20].plot(kind = 'barh')

top_cities=df['City'].value_counts().head(10)

plt.figure(figsize=(10,6))
plt.pie(top_cities,labels=top_cities.index,autopct='%.2f%%')
plt.title("Cities in US reported most accident cases",size=17,color="grey")

import seaborn as sns
sns.set_style("darkgrid")

sns.histplot(cities_by_accident,log_scale=True)

"""Over 1200 cities have reported only one accident during the entire period. This could either be very good news or it could be a result of missing data. Letâ€™s break cities by accidents into two groups and see their respective distributions-

- High accident cities where the number of accidents is greater and equal to 1000.
- Low accident cities where the number is less than 1000.
"""

high_accident_cities = cities_by_accident[cities_by_accident >= 1000]
low_accident_cities = cities_by_accident[cities_by_accident < 1000]

len(high_accident_cities)*100/len(cities)

"""4.24% of the total number of cities have an accident number greater than 1000."""

sns.histplot(high_accident_cities,log_scale = True)

"""Less than 150 cities (out of 11682) reported more than 10000 accidents during the period between February and December .

Similarly, the cities with the low number of accidents are picturized as-
"""

len(low_accident_cities)*100/len(cities)

"""95.74% of the total number of cities have an accident number less than 1000"""

sns.histplot(low_accident_cities,log_scale = True)

"""## Street"""

street_df = pd.DataFrame(df['Street'].value_counts()).reset_index().rename(columns={'index':'Street No.', 'Street':'Cases'})

top_streets=street_df[:20]
top_streets.columns=["street_name","number_of_accidents"]

plt.figure(figsize=(20,6))
sns.barplot(x=top_streets["street_name"],y=top_streets["number_of_accidents"])
plt.xticks(rotation=90)
plt.title("TOP 20 STREETS WITH MAXIMUM NUMBER OF ACCIDENTS ",fontsize=20)
plt.show()

"""- In last 5 years (2016-2020) Street No. I-5 N is having the highest road accidents records.
- In Street No. I-5 N, daily 14 accidents occurred in average.
"""

def street_cases_percentage(val, operator):
    if operator == '=':
        val = street_df[street_df['Cases']==val].shape[0]
    elif operator == '>':
        val = street_df[street_df['Cases']>val].shape[0]
    elif operator == '<':
        val = street_df[street_df['Cases']<val].shape[0]
    print('{:,d} Streets, {}%'.format(val, round(val*100/street_df.shape[0], 2)))
    
    
street_cases_percentage(1, '=')
street_cases_percentage(100, '<')
street_cases_percentage(1000, '<')
street_cases_percentage(1000, '>')
street_cases_percentage(5000, '>')

"""In Our dataset, there are total 93,048 Streets enlisted for accidental cases,

- There are 36,441 Streets (39%) in US which have only 1 accident record in past 5 years.
-  98% Streets of US, have less than 100 road accident cases.
- Only 0.2% Streets in US have the accident cases greater than 1000.
- In last 5 years record of road accidents, only 24 Streets (0.03%) have greater than 5000 cases.

## Weather
"""

df.Weather_Condition.unique()

df_weather = df.groupby('Weather_Condition')[['ID']].count()
df_weather_20 = df_weather.sort_values('ID',ascending=False).head(20)
df_weather_20

sns.barplot(x='ID', y=df_weather_20.index, data=df_weather_20, orient='h');

"""- From graph and top 20 dataframe, most of the accidents happened in Fair weather condition.
- It is very surprising that most of the accidents happened in Fair weather.
"""

for x in np.arange(1, 5):
    df_x = df.loc[df["Severity"] == x]
    if len(df_x) > 0:
        plt.subplots(figsize=(22, 6))
        df_x['Weather_Condition'].value_counts().sort_values(ascending=False).head(20).plot.bar(width=0.5, color='y', edgecolor='k', align='center', linewidth=1)
        plt.xlabel('Weather Condition', fontsize=16)
        plt.ylabel('Accident Count', fontsize=16)
        plt.title('20 of The Main Weather Conditions for Accidents of Severity ' + str(x), fontsize=16)
        plt.xticks(fontsize=16)
        plt.yticks(fontsize=16)
        plt.show()

"""## Severity"""

accidents_severity = df.groupby('Severity').count()['ID']
accidents_severity

fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(aspect="equal"))
labels = [str(i) for i in range(1, len(accidents_severity) + 1)]
plt.pie(accidents_severity, labels=labels, autopct='%1.1f%%', pctdistance=0.85)
circle = plt.Circle( (0,0), 0.7, color='white')
p = plt.gcf()
p.gca().add_artist(circle)
ax.set_title("Accident by Severity",fontdict={'fontsize': 16})
plt.tight_layout()
plt.show()

"""4.6% of the accidents recorded were found to be very severe

## Distance
"""

severity_distance = df.groupby("Severity").mean()["Distance(mi)"].sort_values(ascending=False)

plt.figure(figsize=(18, 8))
plt.title("Medium distance by severity")
sns.barplot(x = severity_distance.values, y = severity_distance.index, orient="h", order=severity_distance.index)
plt.xlabel("Distance (mi)")
plt.show()

"""In this graph we can see that the distance of the accident is more or less proportional to the severity, and in fact accidents with severity 4 have the longest distance.

## Start_time
"""

# Filter out invalid datetime values
df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce').dropna()

counts = pd.to_datetime(df['Start_Time']).dt.day_name().value_counts()
weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]

plt.figure(figsize=(20, 8))
plt.title("Number of accidents for each weekday")
sns.barplot(x=counts.index, y=counts.values, order=weekdays)
plt.xlabel("Weekday")
plt.ylabel("Value")
plt.show()

"""As we can see from the plot above, the days with the most accidents are working days, while in the weekend we have a frequency of at least 2/3 less. This may be due to the fact that during the weekend there are fewer cars on the road.

# Data Preprocessing

## Correlation Matrix
"""

# Cast Start_Time to datetime

df["Start_Time"] = pd.to_datetime(df["Start_Time"])

# Extract year, month, weekday and day
df["Year"] = df["Start_Time"].dt.year
df["Month"] = df["Start_Time"].dt.month
df["Weekday"] = df["Start_Time"].dt.weekday
df["Day"] = df["Start_Time"].dt.day

# Extract hour and minute
df["Hour"] = df["Start_Time"].dt.hour
df["Minute"] = df["Start_Time"].dt.minute

df.head()

"""We decided to decompose the Start_Time feature in year, month, day, weekday, hour and minute, in order to feed them to the models.


"""

corr_matrix = df.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(corr_matrix, vmin=-1, vmax=1, cmap="seismic")
plt.gca().patch.set(hatch="X", edgecolor="#666")
plt.show()

"""From the matrix we can see that the start and end GPS coordinates of the accidents are highly correlated.

In fact, from the medium distance shown before, the end of the accident is usually close to the start, so we can consider just one of them for the machine learning models.

Moreover, the wind chill (temperature) is directly proportional to the temperature, so we can also drop one of them.

We can also see that the presence of a traffic signal is slightly correlated to the severity of an accident meaning that maybe traffic lights can help the traffic flow when an accident occurs.

From the matrix we can also note that we couldn't compute the covariance with Turning_Loop, and that's because it's always False.

## Feature Selection

From the observations made with the correlation matrix, we are going to drop the following features:
- End_Lat and End_Lng
- Wind Chill

Moreover, we are going to drop the following features:
- ID, Source: since they don't carry any information for the severity
- TMC: because it could already contains information about the accident severity
- Start_Time: because it was decomposed by the time features added before (day, month, weekday)
- End_Time: beause we cannot know in advance when the traffic flow will become regular again
- Description: most description only report the name of the road of the accident, and so we decided to omit this feature for semplicity
- Number, Street, County, State, Zipcode, Country: because we just focus on the City where the accident happened
- Timezone, Airport_Code, Weather_Timestamp: because they are not useful for our task
- Turning_Loop: since it's always False
- Sunrise_Sunset, Nautical_Twilight, Astronomical_Twilight: because they are redundant
"""

features_to_drop = ["ID", "Start_Time", "End_Time", "End_Lat", "End_Lng", "Number","Description", "Street", "County", "State", "Zipcode", "Country", "Timezone", "Airport_Code", "Weather_Timestamp", "Wind_Chill(F)", "Turning_Loop", "Sunrise_Sunset", "Nautical_Twilight", "Astronomical_Twilight"]
df = df.drop(features_to_drop, axis=1)
df.head()

print("Number of rows:", len(df.index))
df.drop_duplicates(inplace=True)
print("Number of rows after drop of duplicates:", len(df.index))

"""## Drop duplicates"""

print("Number of rows:", len(df.index))
df.drop_duplicates(inplace=True)
print("Number of rows after drop of duplicates:", len(df.index))

"""## Handle erroneous and missing values"""

df["Side"].value_counts()

"""We can see that there is one record without side, so we can drop it."""

df = df[df["Side"] != " "]
df["Side"].value_counts()

"""Let's instead analyze Pressure and Visibility:"""

df[["Pressure(in)", "Visibility(mi)"]].describe().round(2)

"""We can see that the minimum value is 0, meaning that some records are missing them and replaced them by putting zeros. For this reason, we are going to drop the records with missing values for these two columns."""

df = df[df["Pressure(in)"] != 0]
df = df[df["Visibility(mi)"] != 0]
df[["Pressure(in)", "Visibility(mi)"]].describe().round(2)

"""If we analyze the weather conditions, we can see that there are lots of them, so it's better to reduce the number of unique conditions.

## Outlier Treatment
"""

# Removing outliers | Wind_Speed(mph)
# Removing records wind speed more than 260 mph (since higher than ~253mph is not recorded yet)

df.drop(df[df['Wind_Speed(mph)'] > 260].index, inplace=True)

# Removing outliers | Distance(mi)
# Removing records distance(mi) more than 109 miles (since higher than ~109 miles is not recorded yet)

df.drop(df[df['Distance(mi)'] > 109].index, inplace=True)

# Removing outliers | Temperature(F)
# Removing records temperature(f) more than 131.4 mph (since higher than ~134.1 F is not recorded yet)

df.drop(df[df['Temperature(F)'] > 134.1].index, inplace=True)

# Removing outliers | Pressure(in)
# Removing records pressure(in) less than 25.69 (since lesser than ~25.69 is not recorded yet)
# Removing records pressure(in) more than 32.03 (since higher than ~32.03 is not recorded yet)

df.drop(df[df['Pressure(in)'] < 25.69].index, inplace=True)
df.drop(df[df['Pressure(in)'] > 32.03].index, inplace=True)

# Removing outliers | Visibility
# Removing records visibility(mi) more than 150 miles (since higher than ~150 miles is not recorded yet)

df.drop(df[df['Visibility(mi)'] > 150].index, inplace=True)

unique_weather = df["Weather_Condition"].unique()

print(len(unique_weather))
print(unique_weather)

"""To do so, we are going to replace them with a more generic description:"""



"""## Combining

If we analyze the weather conditions, we can see that there are lots of them, so it's better to reduce the number of unique conditions.
"""

unique_weather = df["Weather_Condition"].unique()

print(len(unique_weather))
print(unique_weather)

df.loc[df["Weather_Condition"].str.contains("Thunder|T-Storm", na=False), "Weather_Condition"] = "Thunderstorm"
df.loc[df["Weather_Condition"].str.contains("Snow|Sleet|Wintry", na=False), "Weather_Condition"] = "Snow"
df.loc[df["Weather_Condition"].str.contains("Rain|Drizzle|Shower", na=False), "Weather_Condition"] = "Rain"
df.loc[df["Weather_Condition"].str.contains("Wind|Squalls", na=False), "Weather_Condition"] = "Windy"
df.loc[df["Weather_Condition"].str.contains("Hail|Pellets", na=False), "Weather_Condition"] = "Hail"
df.loc[df["Weather_Condition"].str.contains("Fair", na=False), "Weather_Condition"] = "Clear"
df.loc[df["Weather_Condition"].str.contains("Cloud|Overcast", na=False), "Weather_Condition"] = "Cloudy"
df.loc[df["Weather_Condition"].str.contains("Mist|Haze|Fog", na=False), "Weather_Condition"] = "Fog"
df.loc[df["Weather_Condition"].str.contains("Sand|Dust", na=False), "Weather_Condition"] = "Sand"
df.loc[df["Weather_Condition"].str.contains("Smoke|Volcanic Ash", na=False), "Weather_Condition"] = "Smoke"
df.loc[df["Weather_Condition"].str.contains("N/A Precipitation", na=False), "Weather_Condition"] = np.nan

print(df["Weather_Condition"].unique())

"""Let's check also the Wind_Direction field:"""

df["Wind_Direction"].unique()

"""As we can see, we can group the values like we did with Weather_Condition:"""

df.loc[df["Wind_Direction"] == "CALM", "Wind_Direction"] = "Calm"
df.loc[df["Wind_Direction"] == "VAR", "Wind_Direction"] = "Variable"
df.loc[df["Wind_Direction"] == "East", "Wind_Direction"] = "E"
df.loc[df["Wind_Direction"] == "North", "Wind_Direction"] = "N"
df.loc[df["Wind_Direction"] == "South", "Wind_Direction"] = "S"
df.loc[df["Wind_Direction"] == "West", "Wind_Direction"] = "W"

df["Wind_Direction"] = df["Wind_Direction"].map(lambda x : x if len(x) != 3 else x[1:], na_action="ignore")

df["Wind_Direction"].unique()

"""Next, let's analyze the missing values:"""

df.isna().sum()

"""Since a lot of records do not have informations about Precipitation, we are going to drop the feature.

For numerical features we are going to fill the missing features with the mean, while for categorical features like City, Wind_Direction, Weather_Condition and Civil_Twilight, we are going to delete the records with missing informations.
"""

features_to_fill = ["Temperature(F)", "Humidity(%)", "Pressure(in)", "Visibility(mi)", "Wind_Speed(mph)", "Precipitation(in)"]
df[features_to_fill] = df[features_to_fill].fillna(df[features_to_fill].mean())

df.dropna(inplace=True)

df.isna().sum()

"""## Feature scaling

In this section we are going to scale and normalize the features.

To improve the performance of our models, we normalized the values of the continuous features.
"""

df.columns

scaler = MinMaxScaler()
features = ['Temperature(F)','Distance(mi)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)','Start_Lng','Start_Lat','Year', 'Month','Weekday','Day','Hour','Minute']
df[features] = scaler.fit_transform(df[features])
df.head()

"""## Feature encoding

Finally, in this section we are going to encode the categorical features.
"""

categorical_features = set(["Side", "City", "Wind_Direction", "Weather_Condition", "Civil_Twilight"])

for cat in categorical_features:
    df[cat] = df[cat].astype("category")

df.info()

"""First of all, we show the number of unique classes for each categorical feature."""

print("Unique classes for each categorical feature:")
for cat in categorical_features:
    print("{:15s}".format(cat), "\t", len(df[cat].unique()))

"""Let's first encode the boolean values in a numerical form"""

df = df.replace([True, False], [1, 0])

df.head()

"""Now we can encode the categorical features using the method get_dummies() which converts the features with the one-hot encoding."""

# Remove city because it will be encoded later
onehot_cols = categorical_features - set(["City"])

df = pd.get_dummies(df, columns=onehot_cols, drop_first=True)

df.head()

"""Now, remains only to encode the City feature. In order to, reduce the usage of memory and the number of features we used the BinaryEncoder included in the library category_encoders."""

pip install category_encoders

import category_encoders as ce
binary_encoder = ce.binary.BinaryEncoder()

city_binary_enc = binary_encoder.fit_transform(df["City"])
city_binary_enc

"""Finally, we can merge the two dataframes and obtain the final dataframe X with the categorical features encoded."""

df = pd.concat([df, city_binary_enc], axis=1).drop("City", axis=1)

df.head()

"""# Test_Train Split"""

temp1 = df
temp2 = df
temp3 = df
temp4 = df

# df.shape
temp1.shape

temp1 = temp1.sample(frac=0.04, random_state=42)
temp2 = temp2.sample(frac=0.02, random_state=42)
temp3 = temp3.sample(frac=0.01, random_state=42)

y = temp1['Severity'].copy()
X = temp1.drop('Severity', axis=1).copy()

y_small = temp3['Severity'].copy()
X_small = temp3.drop('Severity', axis=1).copy()

y.unique()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, test_size=0.2, random_state=42)

X_train.shape

X_train_small.shape

X_test.shape

y_train

"""# Models"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import metrics
from sklearn.impute import MissingIndicator, SimpleImputer
from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder
import statsmodels.formula.api as smf
import statsmodels.tsa as tsa
from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor 
from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error

"""## Decision Tree"""

DT = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')
DT.fit(X_train, y_train)
dt_pred = DT.predict(X_test)

print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, dt_pred)))

print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, dt_pred,average='micro')))

mse = mean_squared_error(y_test, dt_pred)
print("Mean Squared Error of Decision tree model is : {:.4f}".format(mse))

"""## KNN"""

n = 7
KNN = KNeighborsClassifier(n_neighbors = n)
KNN.fit(X_train, y_train)
knn_pred = KNN.predict(X_test)

print('Accuracy score of the K-Nearest Neighbors model is {}'.format(accuracy_score(y_test, knn_pred)))

print('F1 score of the KNN model is {}'.format(f1_score(y_test, knn_pred,average='micro')))

mse = mean_squared_error(y_test, knn_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""## Logistic Regression"""

lr = LogisticRegression()
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)

print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_pred)))

print('F1 score of the Logistic regression model is {}'.format(f1_score(y_test, lr_pred,average='micro')))

mse = mean_squared_error(y_test, lr_pred)
print("Mean Squared Error of Logistic regression model is : {:.4f}".format(mse))

"""## SVM"""

svm = SVC()
svm.fit(X_train_small, y_train_small)
svm_pred = svm.predict(X_test_small)

print('Accuracy score of the Support Vector Machines model is {}'.format(accuracy_score(y_test_small, svm_pred)))

print('F1 score of the SVM model is {}'.format(f1_score(y_test_small, svm_pred,average='micro')))

mse = mean_squared_error(y_test_small, svm_pred)
print("Mean Squared Error of SVM model is : {:.4f}".format(mse))

"""## Random Forest"""

rf = RandomForestClassifier(max_depth = 4)
rf.fit(X_train_small, y_train_small)
rf_pred = rf.predict(X_test_small)

print('Accuracy score of the Random Forest model is {}'.format(accuracy_score(y_test_small, rf_pred)))

print('F1 score of the Random Forest model is {}'.format(f1_score(y_test_small, rf_pred,average='micro')))

mse = mean_squared_error(y_test_small, rf_pred)
print("Mean Squared Error of Random Forest model is : {:.4f}".format(mse))

"""## XGBoost"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train_xgb = le.fit_transform(y_train)
y_test_xgb = le.fit_transform(y_test)

xgb = XGBClassifier(max_depth = 4)
xgb.fit(X_train, y_train_xgb)
xgb_pred = xgb.predict(X_test)

print('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test_xgb, xgb_pred)))

print('F1 score of the XGBoost model is {}'.format(f1_score(y_test_xgb, xgb_pred,average='micro')))

mse = mean_squared_error(y_test, xgb_pred)
print("Mean Squared Error of XGBoost model is : {:.4f}".format(mse))

"""##ADA Boost"""

ada = AdaBoostClassifier()
ada.fit(X_train,y_train)
ada_preds = ada.predict(X_test)

print('Accuracy score of the AdaBoost model is {}'.format(accuracy_score(y_test, ada_preds)))

print('F1 score of the AdaBoost model is {}'.format(f1_score(y_test, ada_preds,average='weighted')))

mse = mean_squared_error(y_test, ada_preds)

print("Mean Squared Error of AdaBoost model is : {:.4f}".format(mse))

"""#Pre-process the data for location prediction

from the dataset we dropped the features which are not useful and which are meaningless and for meaningless prediction of location


*   ID: all unique features
*   Description, Number, Street, Side, Country, State, Zipcode, Timezone,  Airportcode: all these are location indicator for which location prediction will become meaningless.
*   Remaining dropped features: meaningless for location prediction
"""

df = pd.read_csv("/content/drive/MyDrive/US_Accidents_Dec21_updated.csv")

features_for_drop = ['ID','Description','Number','Street','Side','County','State','Zipcode','Country','Timezone','Airport_Code',
                     'Weather_Timestamp','Start_Time','End_Time','End_Lat','End_Lng','Sunrise_Sunset','Nautical_Twilight', 
                     'Astronomical_Twilight','Precipitation(in)','Civil_Twilight']
df = df.drop(features_for_drop, axis=1)
df.head()

"""Drop duplicates"""

print("Number of rows:", len(df.index))
df.drop_duplicates(inplace=True)
print("Number of rows after drop of duplicates:", len(df.index))

"""##Handle erroneous and missing values"""

df[["Pressure(in)", "Visibility(mi)"]].describe().round(2)

df = df[df["Pressure(in)"] != 0]
df = df[df["Visibility(mi)"] != 0]
df[["Pressure(in)", "Visibility(mi)"]].describe().round(2)

"""##Outlier Treatment"""

df.drop(df[df['Wind_Speed(mph)'] > 260].index, inplace=True)
df.drop(df[df['Distance(mi)'] > 109].index, inplace=True)
df.drop(df[df['Temperature(F)'] > 134.1].index, inplace=True)
df.drop(df[df['Pressure(in)'] < 25.69].index, inplace=True)
df.drop(df[df['Pressure(in)'] > 32.03].index, inplace=True)
df.drop(df[df['Visibility(mi)'] > 150].index, inplace=True)
unique_weather = df["Weather_Condition"].unique()

print(len(unique_weather))
print(unique_weather)

"""##Combinations"""

df.loc[df["Weather_Condition"].str.contains("Thunder|T-Storm", na=False), "Weather_Condition"] = "Thunderstorm"
df.loc[df["Weather_Condition"].str.contains("Snow|Sleet|Wintry", na=False), "Weather_Condition"] = "Snow"
df.loc[df["Weather_Condition"].str.contains("Rain|Drizzle|Shower", na=False), "Weather_Condition"] = "Rain"
df.loc[df["Weather_Condition"].str.contains("Wind|Squalls", na=False), "Weather_Condition"] = "Windy"
df.loc[df["Weather_Condition"].str.contains("Hail|Pellets", na=False), "Weather_Condition"] = "Hail"
df.loc[df["Weather_Condition"].str.contains("Fair", na=False), "Weather_Condition"] = "Clear"
df.loc[df["Weather_Condition"].str.contains("Cloud|Overcast", na=False), "Weather_Condition"] = "Cloudy"
df.loc[df["Weather_Condition"].str.contains("Mist|Haze|Fog", na=False), "Weather_Condition"] = "Fog"
df.loc[df["Weather_Condition"].str.contains("Sand|Dust", na=False), "Weather_Condition"] = "Sand"
df.loc[df["Weather_Condition"].str.contains("Smoke|Volcanic Ash", na=False), "Weather_Condition"] = "Smoke"
df.loc[df["Weather_Condition"].str.contains("N/A Precipitation", na=False), "Weather_Condition"] = np.nan

print(df["Weather_Condition"].unique())

df["Wind_Direction"].unique()

df.loc[df["Wind_Direction"] == "CALM", "Wind_Direction"] = "Calm"
df.loc[df["Wind_Direction"] == "VAR", "Wind_Direction"] = "Variable"
df.loc[df["Wind_Direction"] == "East", "Wind_Direction"] = "E"
df.loc[df["Wind_Direction"] == "North", "Wind_Direction"] = "N"
df.loc[df["Wind_Direction"] == "South", "Wind_Direction"] = "S"
df.loc[df["Wind_Direction"] == "West", "Wind_Direction"] = "W"

df["Wind_Direction"] = df["Wind_Direction"].map(lambda x : x if len(x) != 3 else x[1:], na_action="ignore")

df["Wind_Direction"].unique()

df.isna().sum()

features_to_fill = ["Temperature(F)", "Humidity(%)", "Pressure(in)", "Visibility(mi)", "Wind_Speed(mph)", "Wind_Chill(F)"]
df[features_to_fill] = df[features_to_fill].fillna(df[features_to_fill].mean())

df.dropna(inplace=True)

df.isna().sum()

scaler = MinMaxScaler()
features = ['Temperature(F)','Distance(mi)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Wind_Chill(F)','Severity']
df[features] = scaler.fit_transform(df[features])
df.head()

"""##Feature encoding"""

categorical_features = set(["Wind_Direction", "Weather_Condition","City"])

for cat in categorical_features:
    df[cat] = df[cat].astype("category")

df.info()

print("Unique classes for each categorical feature:")
for cat in categorical_features:
    print("{:15s}".format(cat), "\t", len(df[cat].unique()))

df = df.replace([True, False], [1, 0])

df.head()

# Remove city because it will be encoded later
onehot_cols = categorical_features - set(["City"])

df = pd.get_dummies(df, columns=onehot_cols, drop_first=True)

pip install category_encoders

import category_encoders as ce
binary_encoder = ce.binary.BinaryEncoder()

city_binary_enc = binary_encoder.fit_transform(df["City"])
city_binary_enc

df = pd.concat([df, city_binary_enc], axis=1).drop("City", axis=1)

df.head()

df[["Start_Lat", "Start_Lng"]].describe().round(2)

"""#Do Predictable Classification of Target Features"""

df['Start_Lat'] = df['Start_Lat'].astype(int)  
# Define a function to map values to ranges
def map1_to_range(i):
    if i >= 24 and i < 30:
        return 1
    elif i >= 30 and i < 40:
        return 2
    else:
        return 3

# Apply the function to the column of interest
df['Start_Lat'] = df['Start_Lat'].apply(map1_to_range)
df['Start_Lng'] = df['Start_Lng'].astype(int) 
def map2_to_range(j):
    if j >= -125 and j < -112:
        return 4
    elif j >= -112 and j < -90:
        return 3
    elif j >= -90 and j < -78:
        return 2
    else:
        return 1

# Apply the function to the column of interest
df['Start_Lng'] = df['Start_Lng'].apply(map2_to_range) 
temp1 = df
temp2 = df
temp3 = df
temp4 = df
# df.shape
temp1.shape

"""#Train-Test Split for start latitude prediction"""

temp1 = temp1.sample(frac=0.04, random_state=42)
temp2 = temp2.sample(frac=0.02, random_state=42)
temp3 = temp3.sample(frac=0.01, random_state=42)
y1 = temp1['Start_Lat'].copy()

X1 = temp1.drop('Start_Lat', axis=1).copy()

y1_small = temp3['Start_Lat'].copy()
X1_small = temp3.drop('Start_Lat', axis=1).copy()

y1.unique()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
scaler = StandardScaler()
X1 = scaler.fit_transform(X1)

X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)
X1_train_small, X1_test_small, y1_train_small, y1_test_small = train_test_split(X1_small, y1_small, test_size=0.2, random_state=42)
X1_train.shape

X1_train_small.shape
X1_test.shape

y1_train

"""#Apply Models for predictions of Start Latitude

##Decision tree
"""

DT = DecisionTreeClassifier(max_depth=5, criterion='entropy')
DT.fit(X1_train, y1_train)
dt1_pred = DT.predict(X1_test)

print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y1_test, dt1_pred)))

print('F1 score of the Decision Tree model is {}'.format(f1_score(y1_test, dt1_pred,average='micro')))

mse = mean_squared_error(y1_test, dt1_pred)
print("Mean Squared Error of Decision Tree model is : {:.4f}".format(mse))

"""##KNN"""

n = 3
KNN = KNeighborsClassifier(n_neighbors = n)
KNN.fit(X1_train, y1_train)
knn1_pred = KNN.predict(X1_test)

print('Accuracy score of the K-Nearest Neighbors model is {}'.format(accuracy_score(y1_test, knn1_pred)))

print('F1 score of the KNN model is {}'.format(f1_score(y1_test, knn1_pred,average='micro')))

mse = mean_squared_error(y1_test, knn1_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##Logistic regression"""

lr = LogisticRegression()
lr.fit(X1_train, y1_train)
lr1_pred = lr.predict(X1_test)

print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y1_test, lr1_pred)))

print('F1 score of the Logistic regression model is {}'.format(f1_score(y1_test, lr1_pred,average='micro')))

mse = mean_squared_error(y1_test, lr1_pred)

print("Mean Squared Error of LR model is : {:.4f}".format(mse))

"""##SVC"""

svm = SVC()
svm.fit(X1_train_small, y1_train_small)
svm1_pred = svm.predict(X1_test_small)

print('Accuracy score of the Support Vector Machines model is {}'.format(accuracy_score(y1_test_small, svm1_pred)))

print('F1 score of the SVM model is {}'.format(f1_score(y1_test_small, svm1_pred,average='micro')))

mse = mean_squared_error(y1_test_small, svm1_pred)

print("Mean Squared Error of SVM model is : {:.4f}".format(mse))

"""##Random Forest"""

rf = RandomForestClassifier(max_depth = 5)
rf.fit(X1_train_small, y1_train_small)
rf1_pred = rf.predict(X1_test_small)

print('Accuracy score of the Random Forest model is {}'.format(accuracy_score(y1_test_small, rf1_pred)))

print('F1 score of the Random Forest model is {}'.format(f1_score(y1_test_small, rf1_pred,average='micro')))

mse = mean_squared_error(y1_test_small, rf1_pred)

print("Mean Squared Error of RF model is : {:.4f}".format(mse))

"""##XGBoost"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y1_train_xgb = le.fit_transform(y1_train)
y1_test_xgb = le.fit_transform(y1_test)

xgb = XGBClassifier(max_depth = 5)
xgb.fit(X1_train, y1_train_xgb)
xgb1_pred = xgb.predict(X1_test)

print('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y1_test_xgb, xgb1_pred)))

print('F1 score of the XGB model is {}'.format(f1_score(y1_test_xgb, xgb1_pred,average='micro')))

mse = mean_squared_error(y1_test_xgb, xgb1_pred)

print("Mean Squared Error of XGB model is : {:.4f}".format(mse))

"""##ADA Boost"""

ada = AdaBoostClassifier()
ada.fit(X1_train,y1_train)
ada1_preds = ada.predict(X1_test)

print('Accuracy score of the AdaBoost model is {}'.format(accuracy_score(y1_test, ada1_preds)))

print('F1 score of the AdaBoost model is {}'.format(f1_score(y1_test, ada1_preds,average='weighted')))

mse = mean_squared_error(y1_test, ada1_preds)

print("Mean Squared Error of AdaBoost model is : {:.4f}".format(mse))

"""#Train-Test Split for start longitude prediction"""

y2 = temp1['Start_Lng'].copy()
X2 = temp1.drop('Start_Lng', axis=1).copy()
y2_small = temp3['Start_Lng'].copy()
X2_small = temp3.drop('Start_Lng', axis=1).copy()

scaler = StandardScaler()
X2 = scaler.fit_transform(X2)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)
X2_train_small, X2_test_small, y2_train_small, y2_test_small = train_test_split(X2_small, y2_small, test_size=0.2, random_state=42)
X2_train.shape

X2_train_small.shape

X2_test.shape

y2_train

"""#Apply Models for prediction of start Longitude predictions

##Decision tree
"""

DT = DecisionTreeClassifier(max_depth=5, criterion='entropy')
DT.fit(X2_train, y2_train)
dt2_pred = DT.predict(X2_test)

print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y2_test, dt2_pred)))

print('F1 score of the Decision Tree model is {}'.format(f1_score(y2_test, dt2_pred,average='micro')))

mse = mean_squared_error(y2_test, dt2_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##KNN"""

n = 4
KNN = KNeighborsClassifier(n_neighbors = n)
KNN.fit(X2_train, y2_train)
knn2_pred = KNN.predict(X2_test)

print('Accuracy score of the K-Nearest Neighbors model is {}'.format(accuracy_score(y2_test, knn2_pred)))

print('F1 score of the KNN model is {}'.format(f1_score(y2_test, knn2_pred,average='micro')))

mse = mean_squared_error(y2_test, knn2_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##Logistic Regression"""

lr = LogisticRegression()
lr.fit(X2_train, y2_train)
lr2_pred = lr.predict(X2_test)

print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y2_test, lr2_pred)))

print('F1 score of the LR model is {}'.format(f1_score(y2_test, lr2_pred,average='micro')))

mse = mean_squared_error(y2_test, lr2_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##SVM"""

svm = SVC()
svm.fit(X2_train_small, y2_train_small)
svm2_pred = svm.predict(X2_test_small)

print('Accuracy score of the Support Vector Machines model is {}'.format(accuracy_score(y2_test_small, svm2_pred)))

print('F1 score of the SVM model is {}'.format(f1_score(y2_test_small, svm2_pred,average='micro')))

mse = mean_squared_error(y2_test_small, svm2_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##Random Forest"""

rf = RandomForestClassifier(max_depth = 5)
rf.fit(X2_train_small, y2_train_small)
rf2_pred = rf.predict(X2_test_small)

print('Accuracy score of the Random Forest model is {}'.format(accuracy_score(y2_test_small, rf2_pred)))

print('F1 score of the RF model is {}'.format(f1_score(y2_test_small, rf2_pred,average='micro')))

mse = mean_squared_error(y2_test_small, rf2_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##XGBoost"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y2_train_xgb = le.fit_transform(y2_train)
y2_test_xgb = le.fit_transform(y2_test)

xgb = XGBClassifier(max_depth = 4)
xgb.fit(X2_train, y2_train_xgb)
xgb2_pred = xgb.predict(X2_test)

print('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y2_test_xgb, xgb2_pred)))

print('F1 score of the XGBoost model is {}'.format(f1_score(y2_test_xgb, xgb2_pred, average='micro')))

mse = mean_squared_error(y2_test, xgb2_pred)
print("Mean Squared Error of KNN model is : {:.4f}".format(mse))

"""##ADA Boost"""

ada = AdaBoostClassifier()
ada.fit(X2_train,y2_train)
ada2_preds = ada.predict(X2_test)

print('Accuracy score of the AdaBoost model is {}'.format(accuracy_score(y2_test, ada2_preds)))

print('F1 score of the AdaBoost model is {}'.format(f1_score(y2_test, ada2_preds,average='weighted')))

mse = mean_squared_error(y2_test, ada2_preds)

print("Mean Squared Error of AdaBoost model is : {:.4f}".format(mse))

"""# Analysis Summary

The project analyzed various parameters on accident occurrence and the following insights have been found upon drilling the above parameters:

- The top 5 states by accidents include populous ones like California, Florida, Texas, Oregon, Virginia.
- Less than 5% of cities have more than 1000 accidents in the period between February 2016 and December 2021.
- The majority of them have witnessed between 10â€“100 accidents during the period.
- Accidents by cities follow an exponentially decreasing distribution.
- The hypothesis that weekdays see more accidents during morning and evening rush hours is corroborated by the data.On working days of week most of the accidents happened from 7am to 9am, may be beacause of many people goes to office at this time.Also, number of accidents are more from 4pm to 6pm, may be because of it is time of returning from office.
- But on weekend days, distribution of number of accidents is pretty much different from working days of week. (12 pm - 6 pm).
- All the working week days had approx same number of accidents.
- Most of the accidents happened in Fair weather condition
- Upto 80% humidity, number of accidents increased (approx) uniformly with increase in humiidity. Above 80% humidity there is no such relation between number of accidents & humidity.
- Number of accidents increased from 2016 to 2021 & Year 2021 had most accidents.
-  Majority of accidents have severity ~2 means not much higher impact on traffic.
- Most-deadliest accident hour is 5:00PM
- Maximum no of cases occured between temperature range: 50-80 F.
- Wind speed is not the reason for accidents.
- Weather condition was Fair in most of the cases hence it is not a major cause behind the accidents.
- Weather Conditions like 'Fair' and 'Cloudy' has most occuring cases of Severity type 2.
"""